{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptic import pmi_tfidf_classifier as ptah\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from time import time\n",
    "import string\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "def tokenize(s):\n",
    "    return [i for i in word_tokenize(s.lower()) if i not in stop_words]\n",
    "\n",
    "def tokenization(train_data, var_name='abstract'):\n",
    "    tokenized_texts = []\n",
    "    #print(\"Tokenization....\")\n",
    "    for _, row in train_data.iterrows():\n",
    "        text = str(row[var_name])\n",
    "        text = str(row['title']) + ' ' + str(row['abstract']) + ' ' + str(row['tox_annotation'])\n",
    "        words = tokenize(text)\n",
    "        tokenized_texts.append(words)\n",
    "    return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainin time (min): 0.5511654257774353\n",
      "testing time (min): 0.066676394144694\n"
     ]
    }
   ],
   "source": [
    "path = '../datasets/DILI/merged_additional_data_dili_cleaned.csv'\n",
    "\n",
    "data_raw = pd.read_csv(path)\n",
    "indices = np.random.permutation(data_raw.index)\n",
    "data = data_raw.loc[indices]\n",
    "data = data_raw.sample(frac=1)\n",
    "\n",
    "idx = int(data.shape[0] * 0.1)\n",
    "test_data = data.iloc[:idx]\n",
    "train_data = data.iloc[idx:]\n",
    "targets_train = train_data['label'].values\n",
    "targets_test = test_data['label'].values\n",
    "train_data\n",
    "\n",
    "s1 = time()\n",
    "tokenized_texts = tokenization(train_data)\n",
    "N = len(tokenized_texts)\n",
    "word2text_count = ptah.get_word_stat(tokenized_texts)\n",
    "words_pmis = ptah.create_pmi_dict(tokenized_texts, targets_train, min_count=5)\n",
    "e1 = time()\n",
    "\n",
    "s2 = time()\n",
    "tokenized_test_texts = tokenization(test_data)\n",
    "results = ptah.classify_pmi_based(words_pmis, word2text_count, tokenized_test_texts, N)\n",
    "e2 = time()\n",
    "\n",
    "print('trainin time (min):', (e1 - s1) / 60)\n",
    "print('testing time (min):', (e2 - s2) / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9337436640115858\n",
      "precision: 0.9650756693830035\n",
      "recall: 0.8441955193482689\n",
      "f1_score: 0.9005975013579578\n",
      "fp_rate: 0.016853932584269662\n",
      "fn_rate: 0.15580448065173116\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(results, targets_test).ravel()\n",
    "print('accuracy:', accuracy_score(results, targets_test))\n",
    "print('precision:', precision_score(results, targets_test))\n",
    "print('recall:', recall_score(results, targets_test))\n",
    "print('f1_score:', f1_score(results, targets_test))\n",
    "print('fp_rate:', fp / (fp + tn))\n",
    "print('fn_rate:', fn / (fn + tp))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c87cabe562f5e807d26d1cd98e5393fbbae9a42e9f9c4a4ccda557b7b96e2ecb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
